---
title: "LiDAR Simulation, Classification and Mounting"
author: "Steven Lee"
categories: perception
tags: [documentation, lidar, classification, simulation]
image: ## lidar/rviz-os1.gif
published: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
mathjax: true
## classes: wide
---

<!-- * LiDAR pipeline simulation
* LiDAR traffic cone classification
* LiDAR mounting position -->

<!-- Discuss changes in our situation.
Stage 4 so that we cannot enter the labs to test on the Husky.
But we work to our best extent to maximise the outcome.
Work with constraints and get creative. -->

Since the previous blog post update in July, a lot changes have occurred, including the transition into Stage 4 lockdown in Melbourne. This includes curfew from 8pm to 5am, 5km travel limit, and shopping restrictions such that only people with work permits can visit hardware stores.

However, the team has pressed on to make progress wherever possible while taking into account the imposed constraints. The original plan was to head into the laboratory to work on the Husky robot at this time. However, due to this unforeseen circumstance, we ended up having to shift our main focus to simulation within Gazebo. Optimistically, we should still be able to get a few weeks of hands-on time with the Husky robot after Stage 4 restriction ends, but it will still be a challenge to get everything running and collect all the validation data required for final report.

## LiDAR Pipeline Simulation
In order to ensure that the existing LiDAR pipeline can integrate smoothly with the SLAM module, a [simple simulation](https://github.com/MURDriverless/tortoisebot) is created for testing and integration.

<figure>
  <img src="/assets/img/lidar-2/lidar-sim-cones-2.gif" alt="this is a placeholder image">
  <figcaption>TortoiseBot Robot in Gazebo Simulator</figcaption>
</figure>

This simulation is created with reference to [Wil Selby's blog post](https://www.wilselby.com/2019/05/simulating-an-ouster-os-1-lidar-sensor-in-ros-gazebo-and-rviz/) which details the high level steps he took to get OS1-64 LiDAR mounted on a RC car in Gazebo simulator.

By default, the Gazebo simulator uses CPU for LiDAR laser ray simulation, but this is not computationally efficient at all and scales rather badly when simulating high resolution LiDAR such as the Ouster OS1-64. Fortunately, there exists a [GPU ray plugin](http://gazebosim.org/tutorials?tut=ros_gzplugins##GPULaser) that allows users to simulate the LiDAR sensor with GPU. This significantly improves simulation performance and allows the CPU to focus on processing the main LiDAR pipeline.

## LiDAR Cone Classification

While the previous LiDAR pipeline was able to detect traffic cones, it still lacked the capability of differentiating between the blue and yellow traffic cones, which are vital information for the path planner. The blue and yellow traffic cones define the left and right boundaries of the race track respectively.

<figure>
  <img src="/assets/img/lidar-2/cones.png" alt="this is a placeholder image">
  <figcaption>Traffic Cones specified in FSG Competition</figcaption>
</figure>

From the image above, we can see that the traffic cones have distinct visual appearance and patterns. Specifically, the yellow cone follows a **bright-dark-bright** pattern, while the blue cone follows a **dark-bright-dark** pattern. Therefore, even without full 3-channel colour information, we may be able to differentiate between these cone types.

### Approaches Considered

Initially, various approaches were considered for the LiDAR cone classification pipeline. The main options consisted of the following:

1. Use a neural network (such as [PVCNN](https://arxiv.org/abs/1907.03739)) to perform both classification and detection.
2. Build on top of the existing LiDAR pipeline by leveraging other point attributes returned by the OS1 LiDAR.

While option 1 is considered to be more robust, we currently have limited understanding on point cloud based neural networks, and we also lack the dataset which is required for training and testing.

Therefore, we decided to pursue option 2, which does not require significant re-write of the existing pipeline, nor a well constructed dataset which would be time-consuming to develop. This approach uses intensity image generated by the LiDAR to perform classification. The Ouster LiDAR also provides an [`img_node`](https://github.com/ouster-lidar/ouster_example/blob/master/ouster_ros/src/img_node.cpp) which provides a sample on how developer can generate an [image](https://ouster.com/blog/the-camera-is-in-the-lidar/) based on various returned attributes such as intensity, reflectivity, and ambient noise.


### Point Cloud Cluster to Bounding Box

Next, we need to find a way to translate the detected traffic cone point cloud to a bounding box on the intensity image. This was achieved by performing a spherical project of all the points in the point cloud cluster, which can then be further processed to obtain an approximate bounding box on the traffic cone.

<figure>
  <img src="/assets/img/lidar-2/lidar-img-cone-detect-5.gif" alt="this is a placeholder image">
  <figcaption>Traffic Cone Detections on Intensity Image</figcaption>
</figure>

### Data Collection & Classifier Design

A simple convolutional neural network consisting of convolutional layers and max-pool layers was trained to classify between traffic cone intensity image crop-outs. The intensity image crops data were generated by cropping the approximate region of interest and resized to `32 x 32`.


<figure>
  <img src="/assets/img/lidar-2/classifier-net.png" alt="this is a placeholder image">
  <figcaption>Basic CNN Design for LiDAR Image Classifier (Presented in AlexNet Style)</figcaption>
</figure>

However, due to the limited indoor space, only a very limited set of training data was collected. Extensive input data augmentation along with network dropouts were applied, but the network still appeared to have overfitted, which was hinted by the unusually high validation accuracy.

<figure>
  <img src="/assets/img/lidar-2/augmented.png" alt="this is a placeholder image">
  <figcaption>Training data after applying data augmentation</figcaption>
</figure>

Before deployment, the classifier model and weights were saved and converted to an `ONNX` model, which is then optimised for Nvidia's TensorRT framework. This allows the network to perform classification inference within 1ms with a maximum batch size of up to 50.

### Classifier Performance

<figure>
  <img src="/assets/img/lidar-2/lidar-img-pt-cloud-detect-3.gif" alt="this is a placeholder image">
  <figcaption>LiDAR Classification Mini-demo (intensity image on top, point cloud at bottom)</figcaption>
</figure>

## LiDAR Mounting Position


## Future Tasks

* collect more cone crop images to improve traffic cone classifier
* can use existing pipeline to build dataset for other types of detector / classifier that can be directly applied on point cloud data